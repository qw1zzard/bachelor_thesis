\section{Математическая постановка задачи}

В рамках данной главы сначала будет сформулирована математическая постановка рассматриваемой задачи, затем будут описаны широко применяемые парадигмы работы с графовыми данными.

\subsection{Мошенники на сайтах-агрегаторах отзывов}

В этой работе будем определять недобросовестных пользователей как тех, чьи рецензии (которые мы будем считать мошенническими) получают в основном низкие оценки от других пользователей. Это может свидетельствовать о попытках злонамеренно ухудшить репутацию товаров или услуг на открытых интернет-ресурсах. Будет рассмотрена широко известная задача бинарной классификации из области обучения с учителем (supervised learning), где класс 1 соответствует пользователям-мошенникам (или их отзывам, в зависимости от набора), а 0 -– всем остальным. Более подробное описание данных и метрик оценки работы моделей будет представлено в следующей главе.

Стоит отметить, что применимость рассматриваемых архитектур не ограничивается поставленной задачей и может быть успешно адаптирована и под другие темы, например, определение мошеннических банковских транзакций \cite{cheng2023} \cite{jesus2022} \cite{pozzolo2015} и выявление аномалий в файлах записей (логах) компьютерных программ \cite{pan2023}. Однако такие данные обычно закрыты от публичного доступа в целях безопасности и сохранения персональной информации. Особый интерес также представляет сфера криптовалютных переводов, популярная в современном мире \cite{ding2023} \cite{hu2023}, однако хоть объем данных в этой теме и значительно больше, опубликовано при этом сравнительно небольшое количество статей. Во многих перечисленных работах авторы тестировали предложенные подходы на различных задачах, что свидетельствует о широкой применимости графовых нейронных сетей в других областях \cite{li2023} \cite{sotiropoulos2023} \cite{tian2023} \cite{yin2024}.

\pagebreak



\subsection{Пространственная парадигма}

Пространственная парадигма работы с графовыми данными лежит в основе модуля свертки GraphSAGE \cite{hamilton2017}. Ее идея заключается в том, что у каждой вершины есть внутреннее состояние (иначе представление или эмбединг), которое можно обновить по информации из ее окрестности \(N(v)\). Самой известной реализацией такого подхода, предложенной еще до распространения нейросетевых методов, стал алгоритм PageRank, который был предложен и активно использовался компанией Google для ранжирования поисковой выдачи. Для формального описания алгоритма GraphSAGE введем основные понятия, с которыми будем работать на протяжении всей главы.

В общем случае мультиграф принято представлять как набор \(G = \{V, X, \{\mathcal{E}_r\}_{r=1}^R, Y\}\), где \(V\) -- множество узлов \(\{v_1, ..., v_n\}\). Каждая вершина \(v_i\) имеет \(d\)-мерный вектор признаков \(x_i \in \mathbb{R}^d\), а \(X = \{x_1, . . . , x_n\}\) представляет собой множество всех эмбедингов узлов. \(\varepsilon_{ij}^r = (v_i , v_j) \in \mathcal{E}_r\) -- это ребро между \(v_i\) и \(v_j\) с отношением \(r \in \{1, \ldots , R\}\). \(Y\) -- это набор меток для каждой вершины в \(V\), где \(y_v \in \{0, 1\}\) и \(y_v \in Y \). Набор соседей узла \(v\) обычно обозначается как \(N_r(v) = \{\hat{v} \,|\, (\hat{v}, v) \in \mathcal{E}_r\} \). Граф может быть ориентированный или ненаправленный, полносвязный или многодольный, с одним отношением между узлами или множеством, а также гомо- или гетерогенный.

С учетом введенных обозначений, GraphSAGE можно записать в виде следующего алгоритма:

\begin{enumerate}
    \item Для каждого узла \(v\) находится набор скрытых представлений соседних вершин \(h_{N(v)}\), которые смежны с текущей.

    \item Собранная информация агрегируется с помощью коммутативной операции \(AGGR\) (например, взятие средних или максимальных значений эмбедингов) в вектор фиксированного размера.

    \item Полученный вектор объединяется со скрытым представлением вершины \(h_v\) и они домножаются на матрицу \(W\), обучаемую в процессе обратного распространения ошибки (backpropagation).

    \item К результату поэлементно применяется любая нелинейная функция, например, сигмоида:

    \begin{equation}
        \sigma(x) = \frac{1}{1 + e^{-x}}
    \end{equation}
\end{enumerate}
здесь \(h_v^0 = x_v\), то есть множество всех эмбедингов узлов \(X = \{x_1, . . . , x_n\}\) является матрицей входных данных. Компактно алгоритм можно записать в виде следующих двух формул:

\begin{equation}
    m_v^{t+1} = \text{AGGR} \left( \{h_w^t, w \in N(v)\} \right)
\end{equation}

\begin{equation}
    h_v^{t+1} = \sigma \left( W^{t+1} \, \text{CONCAT}(m_v^{t+1},h_v^t) \right)
\end{equation}

Полученная формула свертки использует только скрытые представления вершин и больше внимания уделяет локальному окружению, нежели глобальному положению узла во всем графе. Авторы GraphSAGE \cite{hamilton2017} показали высокое качество работы данной архитектуры в задачах, связанных с выучиванием представлений вершин, однако использование данной свертки можно встретить и в других задачах, связанных с обработкой графов, не содержащих дополнительной информации о ребрах.

В CARE-GNN \cite{dou2020} авторы предложили учитывать только \(p\) соседей узла, чтобы решить проблему чрезмерного усреднения. Они использовали пространственную парадигму при агрегации, но предварительно искали схожие эмбединги в скоплениях вершин с помощью следующей формулы расстояния:

\begin{equation}
    D(l)(v, v') = \left\| \sigma \left( \text{MLP}^{(l)} \left( h_v^{(l-1)} \right) \right) - \sigma \left( \text{MLP}^{(l)} \left( h_{v'}^{(l-1)} \right) \right) \right\|
\end{equation}

\begin{equation}
    S(l)(v, v') = 1 - D(l)(v, v')
\end{equation}
где \( h_v^{(l-1)} \) -- скрытое представление вершины \(v\) на слое \( l-1 \), \( \sigma \) -- любая нелинейная функция активации, например, сигмоида, а \( \text{MLP}^{(l)} \) -- многослойный перцептрон на уровне \( l \).

В NGS \cite{qin2022} для достижения высокой точности объединяется информация с нескольких метаграфов, каждый из которых представляет собой промежуточное состояние исходного на всех слоях нейросети. Сначала к полученным метаграфам применяется \(\text{MLP}\) слой с нелинейной функцией активации \(\text{ReLU}(x) = \max(0, x)\). Затем для каждого узла \(v\) из \(V\) у нас есть набор векторов \(\{h_{M_1}^v, h_{M_2}^v, ..., h_{M_K}^v\}\), где \(K\) -- параметр, заранее задающий количество метаграфов для нахождения. Мы присваиваем векторам различные веса и агрегируем следующим образом:

\begin{equation}
    e_{M_k} = \text{MLP} \left( h_{M_k}^v \right)
\end{equation}

\begin{equation}
    \beta_{M_k} = \frac{\exp\left(e_{M_k}\right)}{\sum_{1\leq k' \leq K} \exp\left(e_{M_k'}\right)}
\end{equation}

\begin{equation}
    h_v = \sum_{1\leq k \leq K} \beta_{M_k} \cdot h_{M_k}^v
\end{equation}
при этом \(h_v\) -- скрытое представление узла, используемое для конечного прогноза, а формула \(\beta_{M_k}\) является многопеременной логистической функцией, которую часто называют softmax.

\pagebreak



\subsection{Спектральная парадигма}

Альтернативой пространственной парадигме является спектральная, основанная на анализе процесса диффузии сигнала внутри графа с использованием матрицы смежности и лапласиана. Этот подход был представлен в работе GCN \cite{kipf2017}, авторы которой сконцентрировались на задаче классификации узлов без использования скрытых представлений ребер, которые часто являются дополнением в GraphSAGE.

Лапласиан графа –- это матрица \(L = D - A\), где диагональная \(D\) хранит в \(i\)-й ячейке количество исходящих из \(v_i\) ребер, а \(A\) –- матрица смежности графа, где элемент \(a_{ij}\) равен числу связей \(\varepsilon_{ij}^r = (v_i , v_j) \in \mathcal{E}_r\).

Лапласиан имеет неотрицательные собственные значения, среди которых количество нулевых всегда совпадает с числом компонент связности. Собственные векторы, соответствующие положительным значениям, описывают разрезы графа -- его разделения пополам так, чтобы между частями было как можно меньше ребер. Этим свойством пользуются для того, чтобы проводить кластеризацию при обучении без учителя (unsupervised learning). Для этого необходимо:

\begin{enumerate}
    \item Вычислить лапласиан \(L\) от матрицы \(A\) графа \(G\).

    \item Найти \(k\) собственных векторов, которые соответствуют наименьшим собственным значениям полученного \(L\).

    \item Из векторов сформировать матрицу размера \(n \times k\), \(i\)-я строка которой будет описывать соответствующую ей вершину \(k\) параметрами.

    \item Кластеризовать объекты, описываемые этой матрицей, например, с помощью алгоритма K-средних (K-means).
\end{enumerate}

Алгоритм GCN очень прост с математической точки зрения и представляет собой один шаг итеративного процесса поиска собственных значений лапласиана графа: скрытые представления вершин домножаются на нормированную матрицу смежности, а также на матричный корень \(D^{-\frac{1}{2}}\):

\begin{equation}
    h^{t+1} = \theta D^{-\frac{1}{2}} (A + I) D^{-\frac{1}{2}} h^t
\end{equation}
где \(h^t\) – это матрица размера (число вершин в графе) \(\times\) (длина вектора представления), то есть к каждому каналу представлений свертка применяется отдельно, а \(\theta\) является обучаемой матрицей весов. Если же мы хотим работать с несколькими каналами, то есть вместо \(h^t\) у нас имеется матрица \(H^t\) размера (число узлов) \(\times\) (число каналов), а также нелинейная функция \(f\), то формула приобретает следующий вид:

\begin{equation}
    H^{t+1} = f\left( D^{-\frac{1}{2}} (A + I) D^{-\frac{1}{2}} H^{t} \Theta \right)
\end{equation}

Таким образом, спектральный подход отлично работает в задачах, связанных с обработкой больших графов, где важно понимать относительное месторасположение вершины в нем. Однако, данная парадигма сталкивается с проблемой эффективного поиска собственных чисел матрицы смежности, а также с невозможностью учитывать скрытые представления ребер, что приводит к выбору других подходов, таких как GraphSAGE или GAT.

\pagebreak



\subsection{Трансформеры}

Так называемый трансформер \cite{vaswani2017}, основанный на концепции внимания (attention) и изначально спроектированный для обработки естественного языка, в последние годы повсеместно применяется и в задачах классификации графов. Объяснить это можно благодаря хорошей масштабируемости и высокому качеству работы практически с любым типом данных.

Модуль GAT \cite{velickovic2017}, использующий в своей основе attention, представляет собой всего две формулы. В первой авторы вычисляют веса для внимания:

\begin{equation}
    \alpha_{v \ast} = \text{softmax} \left(\text{act} \left( \mathbf{a}^T \text{CONCAT} (W h_v^t, W h_\ast^t) \right) \right)
\end{equation}
здесь softmax -- многопеременная логистическая функция, \(\mathbf{a}^T\) -- обучаемый однослойный перцептрон, CONCAT -- конкатенация двух векторов и \(W\) -- обучаемая матрица преобразования, общая для ключей, значений и запросов, а \(h_\ast^t\) является скрытым представлением вершины \(v_\ast \in N(v)\) на слое \(t\). act -– нелинейная функция активации, например, авторы применили:

\begin{equation}
    \text{LeakyReLU}(x) = \begin{cases}
        x, & \text{если } x > 0 \\
        \alpha x, & \text{если } x \leq 0
    \end{cases}
\end{equation}
где \(\alpha \in (0, 1)\). Следующая формула используется для получения скрытых представлений узлов на новом слое с учетом полученных весов внимания:

\begin{equation}
    h_v^{t+1} = \sigma \left( \frac{1}{K} \sum_{k=1}^{K} \sum_{w \in N(v)} \alpha_{vw}^k W^k h_{w}^t \right)
\end{equation}
при этом \(\sigma\) -- нелинейная функция активации и \(K\) -- количество голов внимания, заранее заданный параметр. Авторы используют усреднение найденных эмбедингов вершины, но в качестве альтернативы можно применить любую другую операцию агрегирования.

Множество полученных скрытых представлений узлов на последнем слое нейросети часто называют головой (\(\text{Head}_i\)) модели. В работе GTAN \cite{xiang2023} несколько таких выходов просто объединяют, полагаясь на разную инициализацию начальных весов матрицы обучения \(W\):

\begin{equation}
    H = \text{CONCAT}(\text{Head}_1, \ldots, \text{Head}_{\text{att}}) W_o
\end{equation}
здесь CONCAT -- конкатенация матриц и att -- заранее заданный параметр, определяющий количество голов и размер модели.

В свою очередь, в Exphormer \cite{shirzad2023} исследователи используют скрытые представления не только вершин, но и связей между ними. Так уже приведенная формула внимания дополняется:

\begin{equation}
    \alpha_{v \ast} = \text{softmax} \left(\text{act} \left( \mathbf{a}^T \text{CONCAT} (W h_v^t, W h_\ast^t, W_e e_{v\ast}) \right) \right)
\end{equation}
где \(e_{v\ast}\) -- ребро, инцидентное вершине \(v\) и ее соседям, а \(W_e\) является дополнительной матрицей весов. Соответственно, авторы работают над уменьшением общего числа связей, по которым находится attention, так как для всего графа необходимо будет вычислить эту формулу \(\text{N}^2\) раз, где N -- количество вершин в исходном наборе.

\pagebreak
